{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d7294e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "import os # Configure which GPU\n",
    "if os.getenv(\"CUDA_VISIBLE_DEVICES\") is None:\n",
    "    gpu_num = 0 # Use \"\" to use the CPU\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = f\"{gpu_num}\"\n",
    "\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import sionna as sn\n",
    "import tf2onnx\n",
    "\n",
    "print(sys.executable)\n",
    "print(\"TF:\", tf.__version__)\n",
    "print(\"Sionna:\", sn.__version__)\n",
    "print(\"tf2onnx:\", tf2onnx.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f150d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the TF log level to only show errors\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cccd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6f71d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Sionna\n",
    "import sionna as sn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55b4450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For plotting\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef758a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For saving complex Python data structures efficiently\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dafa84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the implementation of the neural receiver\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f361ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For ONNX / TensorRT export\n",
    "# Optional export imports (handled in separate env)\n",
    "try:\n",
    "    import tf2onnx\n",
    "    import onnx\n",
    "except ImportError:\n",
    "    tf2onnx = None\n",
    "    onnx = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670bc56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducible results\n",
    "sn.phy.config.seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e744eb15-8f12-4975-a2fe-79ee99d9b3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize all possible symbols for a 16-QAM\n",
    "NUM_BITS_PER_SYMBOL = 4\n",
    "mapper = sn.phy.mapping.Mapper(num_bits_per_symbol=NUM_BITS_PER_SYMBOL,\n",
    "                           constellation_type=\"qam\")\n",
    "demapper = sn.phy.mapping.Demapper(demapping_method=\"app\",\n",
    "                               num_bits_per_symbol=NUM_BITS_PER_SYMBOL,\n",
    "                               constellation_type=\"qam\")\n",
    "mapper.constellation.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e012b2-3f4d-4ab3-9e0c-4853aa832260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary source to generate uniform i.i.d. bits\n",
    "binary_source = sn.phy.mapping.BinarySource()\n",
    "\n",
    "# AWGN channel\n",
    "awgn_channel = sn.phy.channel.AWGN()\n",
    "\n",
    "BATCH_SIZE = 128 # How many examples are processed by Sionna in parallel\n",
    "EBN0_DB = 17.0 # Eb/N0 in dB\n",
    "\n",
    "no = sn.phy.utils.ebnodb2no(ebno_db=EBN0_DB,\n",
    "                            num_bits_per_symbol=NUM_BITS_PER_SYMBOL,\n",
    "                            coderate=1.0) # Coderate set to 1 as we do uncoded transmission here\n",
    "\n",
    "bits = binary_source([BATCH_SIZE, 1200]) # Blocklength\n",
    "x = mapper(bits)\n",
    "y = awgn_channel(x, no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce706e97-d560-4735-9f98-198b45745488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize the received symbols\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.axes().set_aspect(1.0)\n",
    "plt.grid(True)\n",
    "plt.scatter(tf.math.real(y), tf.math.imag(y), label='Received')\n",
    "plt.scatter(tf.math.real(x), tf.math.imag(x), label='Transmitted')\n",
    "plt.legend(fontsize=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449f16e8-51dd-45fc-837f-d12ce1ee5d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "bits = np.array([[0,0,0,1]])\n",
    "print(\"Original bits:\", bits)\n",
    "x = mapper(bits)\n",
    "print(\"Complex-valued symbol:\", x.numpy())\n",
    "# Add some noise\n",
    "no = 0.05 # Noise variance\n",
    "y = awgn_channel(x, no)\n",
    "print(\"Received noisy symbol: \", y.numpy())\n",
    "llr = demapper(y, no)\n",
    "print(\"LLRs after demapping:\", llr.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f49dda-f30c-46a2-b49f-3aeb2d4c4bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../../')\n",
    "\n",
    "MAX_LINES = 10000000 # Stop after this many imported lines\n",
    "\n",
    "# Depends on config that was used for data capture\n",
    "fn_input = '../../../data_acquisition/logs/demapper_in.txt'\n",
    "fn_output = '../../../data_acquisition/logs/demapper_out.txt'\n",
    "\n",
    "import os\n",
    "print(os.path.exists(fn_input))\n",
    "print(os.getcwd())\n",
    "\n",
    "# Read training data from data dump\n",
    "def read_training_data(in_file, element_shape):\n",
    "    with open(in_file) as f:\n",
    "        lines = f.readlines()\n",
    "    result = None\n",
    "    i = 0\n",
    "    while i < len(lines):\n",
    "        try:\n",
    "            i = lines.index('QAM16\\n', i)\n",
    "        except ValueError as e:\n",
    "            break\n",
    "        num = int(lines[i+1])\n",
    "        #if num > 30:\n",
    "        #    print(num)\n",
    "        data = np.fromstring(' '.join(lines[i+2:i+2+num]), sep=' ', dtype=np.int16).reshape(num, *element_shape)\n",
    "        if result is None:\n",
    "            result = data\n",
    "        else:\n",
    "            result = np.concatenate((result, data))\n",
    "        i += 2 + num\n",
    "        if i>MAX_LINES:\n",
    "            break\n",
    "    return result\n",
    "\n",
    "def int16_to_float16(symbols_i):\n",
    "    return np.ldexp(symbols_i.astype(np.float32), -8).astype(np.float16)\n",
    "\n",
    "def float16_to_int16(llrs_h):\n",
    "    return np.rint(np.ldexp(llrs_h.astype(np.float32), 8)).astype(np.int16)\n",
    "\n",
    "def norm_int16_to_float16(symbols_i, magnitudes):\n",
    "    args = symbols_i.astype(np.float32)\n",
    "    if magnitudes is not None:\n",
    "        args = args / magnitudes.astype(np.float32)\n",
    "    return np.ldexp(args, 7).astype(np.float16)\n",
    "\n",
    "data_in = read_training_data(fn_input, (2, 2))\n",
    "data_out = read_training_data(fn_output, (4,))\n",
    "assert(data_in.shape[0] == data_out.shape[0])\n",
    "\n",
    "print(\"data_in.shape: \", data_in.shape)\n",
    "print(\"data_out.shape: \", data_out.shape)\n",
    "\n",
    "print(\"First 5 input symbols:\", data_in[0:5,:])\n",
    "print(\"First 5 output symbols:\", data_out[0:5,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5b130b-c98f-4acd-b6a5-83058c89b387",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data_in[:10000, ...]\n",
    "\n",
    "# --- remove divide-by-zero ---\n",
    "H_MIN = 1e-3\n",
    "mask_h = (np.abs(x[:,1,0]) > H_MIN) & (np.abs(x[:,1,1]) > H_MIN)\n",
    "x = x[mask_h]\n",
    "\n",
    "# OAI uses h as decision region between inner and outer bits\n",
    "# We scale the received symbols y back such that y_n = y / h\n",
    "y_n = x[:,0,0]/x[:,1,0] + 1.j*x[:,0,1]/x[:,1,1]\n",
    "\n",
    "# Normalize to Sionna QAM-16 scale\n",
    "s = 0.9486833 - 0.3162278\n",
    "y = y_n * s\n",
    "\n",
    "# --- remove outliers (minimal, magnitude-based) ---\n",
    "R_MAX = 1.5\n",
    "mask_r = np.abs(y) < R_MAX\n",
    "y = y[mask_r]\n",
    "\n",
    "# Average power check\n",
    "avg_power = np.mean(np.abs(y)**2)\n",
    "print(\"Avg. power after norm:\", avg_power)\n",
    "\n",
    "# Plot\n",
    "plt.scatter(y.real, y.imag, label=\"OAI normalized\")\n",
    "plt.title(\"Received QAM symbols (OAI)\")\n",
    "plt.ylabel(\"imaginary part\")\n",
    "plt.xlabel(\"real part\")\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "\n",
    "# Overlay with Sionna constellation\n",
    "c = sn.phy.mapping.Constellation(\"qam\", 4).points.numpy()\n",
    "plt.scatter(c.real, c.imag, label=\"Sionna QAM-16\")\n",
    "plt.legend()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f3663f-7f0d-4cc6-b7b1-7ec3e74320ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "demapper = sn.phy.mapping.Demapper(\"app\", \"qam\", 4)\n",
    "\n",
    "y_tf = tf.constant(y, tf.complex64)\n",
    "# avg_power is signal power ps + no where ps = 1\n",
    "ps = 1. # Signal power\n",
    "no = tf.constant(avg_power-ps, tf.float32)\n",
    "\n",
    "llr = -1* demapper(y_tf, no) # Flip sign due to Sionna's logit definition\n",
    "\n",
    "# Scale llrs\n",
    "llr_ = llr.numpy()\n",
    "\n",
    "llr_ref = np.reshape(llr_.astype(int), (-1,4))\n",
    "\n",
    "llr_oai = data_out[:llr_ref.shape[0],...]\n",
    "\n",
    "# And plot distributions\n",
    "r = np.arange(-100, 100, 1)\n",
    "\n",
    "for bit_idx in range(4):\n",
    "    plt.figure()\n",
    "    plt.hist(llr_ref[:,bit_idx], bins=r, label=\"Sionna APP\")\n",
    "\n",
    "    plt.hist(llr_oai[:,bit_idx], bins=r, alpha=0.3, label=\"OAI demapper\")\n",
    "    plt.legend();\n",
    "    plt.title(f\"LLR distribution bit index {bit_idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51563d9-351e-457e-9502-000931acf6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralDemapper(Model): # Inherits from Keras Layer\n",
    "\n",
    "    def __init__(self, num_bits_per_symbol,):\n",
    "        \"\"\"Neural demapper using a simple 3-layer MLP\n",
    "\n",
    "        Inputs\n",
    "        ------\n",
    "        [y_i, y_q] or [y_i, y_q, h_i, h_q]\n",
    "            list of tensors:\n",
    "\n",
    "        y_i: [batch_size, 1, 1], tf.float32\n",
    "            Real part of the received IQ symbols.\n",
    "\n",
    "        y_q: [batch_size, 1, 1], tf.float32\n",
    "            Imaginary part of the received IQ symbols.\n",
    "\n",
    "        optional:\n",
    "        h_i: [batch_size, 1, 1], tf.float32\n",
    "            Real part of the estimated channel magnitude.\n",
    "\n",
    "        h_q: [batch_size, 1, 1], tf.float32\n",
    "            Real part of the estimated channel magnitude.\n",
    "\n",
    "        Outputs\n",
    "        -------\n",
    "        llr: [batch_size, num_bits_per_symbol], tf.float32\n",
    "            LLRs for each bit of a symbol.\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # We are using three layers, this is not optimized but also not needed for this project\n",
    "        self.dense_1 = Dense(32, 'relu', name='dense_1')\n",
    "        self.dense_2 = Dense(32, 'relu', input_shape=(32,), name='dense_2')\n",
    "\n",
    "        # The last layer has no activation and therefore outputs logits, i.e., LLRs\n",
    "        self.dense_3 = Dense(num_bits_per_symbol, None,\n",
    "                             input_shape=(32,), name='dense_3')\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "\n",
    "        # Supports either a list of inputs or already concatenated version\n",
    "        nn_input = tf.concat(inputs, axis=-1) if isinstance(inputs, list) else inputs\n",
    "        z = self.dense_1(nn_input, training=training)\n",
    "        z = self.dense_2(z, training=training)\n",
    "        llr = self.dense_3(z, training=training) # [batch size, 1, num_bits_per_symbol]\n",
    "\n",
    "        return llr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1d9c93-5c8f-4c3d-a3af-3f7a00109325",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_BITS_PER_SYMBOL = 4 # 16-QAM\n",
    "\n",
    "# Init components\n",
    "neural_demapper_synthetic = NeuralDemapper(NUM_BITS_PER_SYMBOL)\n",
    "\n",
    "# Binary classification problem -> train on BCE loss\n",
    "bce = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "# Use ADAM optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(1e-2)\n",
    "\n",
    "# Dummy run to init the Keras model\n",
    "neural_demapper_synthetic([tf.ones((1,1)),tf.ones((1,1))])\n",
    "\n",
    "neural_demapper_synthetic.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d439dcf-b265-4d37-8cbf-a2debf9ef78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1024\n",
    "NUM_TRAINING_ITERATIONS = 10000\n",
    "\n",
    "# Scaling factor for QAM symbols to be compatible with the OAI demapper\n",
    "qam16_threshold_mag = 2 * 0.3162278\n",
    "\n",
    "@tf.function(jit_compile=False)\n",
    "def training_step():\n",
    "    # we train with a randomized noise variance\n",
    "    no = tf.random.uniform([BATCH_SIZE,1], minval=0., maxval=1.,\n",
    "                           dtype=tf.float32)\n",
    "    # Draw random bits\n",
    "    bits = binary_source([BATCH_SIZE, NUM_BITS_PER_SYMBOL])\n",
    "\n",
    "    # Map to QAM symbols\n",
    "    x = mapper(bits)\n",
    "\n",
    "    # Transmit over Gaussian channel\n",
    "    y = awgn_channel(x, no)\n",
    "\n",
    "    # Split real and imaginary part and scale back to OAI\n",
    "    qxr = tf.math.real(y) * (1.0 / qam16_threshold_mag)\n",
    "    qxi = tf.math.imag(y) * (1.0 / qam16_threshold_mag)\n",
    "\n",
    "    llr = neural_demapper_synthetic([qxr, qxi], training=True)\n",
    "    loss = bce(bits, -llr) # Note: OAI uses flipped logits definition\n",
    "    return loss\n",
    "\n",
    "# Training loop\n",
    "for i in range(NUM_TRAINING_ITERATIONS):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = training_step()\n",
    "    gradient = tape.gradient(loss, tape.watched_variables())\n",
    "    optimizer.apply_gradients(zip(gradient, tape.watched_variables()));\n",
    "    # Print progress\n",
    "    if i % 500 == 0:\n",
    "        print(f\"{i}/{NUM_TRAINING_ITERATIONS}  Loss: {loss:.2E}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b807d3e-141a-475e-9f4b-abee344c0c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bits = np.array([[0,1,1,1]])\n",
    "print(\"Original bits:\", bits)\n",
    "x = mapper(bits)\n",
    "print(\"Complex-valued symbol:\", x.numpy())\n",
    "# Add some noise\n",
    "no = tf.constant([[0.4,]]) # Noise variance\n",
    "y = awgn_channel(x, no)\n",
    "print(\"Received noisy symbol: \", y.numpy())\n",
    "print(y)\n",
    "llr_ref = demapper(y, no)\n",
    "llr_nn = -neural_demapper_synthetic([tf.math.real(y) / qam16_threshold_mag,\n",
    "                                     tf.math.imag(y) / qam16_threshold_mag])\n",
    "print(\"LLRs Reference:\", llr_ref)\n",
    "print(\"LLRs NRX :\", llr_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25691811-a9d2-48c4-baf2-5c2669c1bde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save weights for synthetic demapper\n",
    "fn = \"../../models/neural_demapper_weights_y2\"\n",
    "\n",
    "weights = neural_demapper_synthetic.get_weights()\n",
    "with open(fn, 'wb') as f:\n",
    "    pickle.dump(weights, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bc696c-6da3-4a73-b06a-06d279449b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################\n",
    "# The following code is only relevant if training is done on captured data!\n",
    "###########################################################################\n",
    "\n",
    "# Depends on config that was used for data capture\n",
    "fn_input = '../../../data_acquisition/logs/demapper_in.txt'\n",
    "fn_output = '../../../data_acquisition/logs/demapper_out.txt'\n",
    "\n",
    "# Requires data dump from the previous tutorial\n",
    "demapper_in_data = read_training_data(fn_input, (2, 2))\n",
    "demapper_out_data = read_training_data(fn_output, (4,))\n",
    "assert(demapper_in_data.shape[0] == demapper_out_data.shape[0])\n",
    "\n",
    "print(\"Training data size: \", demapper_out_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda60c94-beb3-4b9f-b588-361e72943212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression loss for training on captured data\n",
    "rge = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "# Use ADAM optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(1e-2)\n",
    "\n",
    "# Init new receiver\n",
    "neural_demapper_capture = NeuralDemapper(NUM_BITS_PER_SYMBOL)\n",
    "\n",
    "# Dummy run to init the Keras model\n",
    "# Note that we now have 4 inputs as we do not scale automatically\n",
    "neural_demapper_capture([tf.ones((1,4))])\n",
    "\n",
    "neural_demapper_capture.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544ba045-063f-4be8-b2b3-31b7e476cd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function(jit_compile=False)\n",
    "def training_step():\n",
    "    # Train on captured data\n",
    "    if True:\n",
    "        i = np.random.randint(0, demapper_in_data.shape[0] - BATCH_SIZE)\n",
    "        llr_ref = demapper_out_data[i:i+BATCH_SIZE]\n",
    "        y_no = demapper_in_data[i:i+BATCH_SIZE]\n",
    "\n",
    "        llr_ref = int16_to_float16(llr_ref)\n",
    "        y_no = int16_to_float16(y_no)\n",
    "\n",
    "        llr = neural_demapper_capture(y_no.reshape(-1, 4), training=True)\n",
    "        loss = rge(llr_ref, llr)\n",
    "        return loss\n",
    "\n",
    "# Training loop\n",
    "for i in range(NUM_TRAINING_ITERATIONS):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = training_step()\n",
    "    gradient = tape.gradient(loss, tape.watched_variables())\n",
    "    optimizer.apply_gradients(zip(gradient, tape.watched_variables()));\n",
    "    # Print progress\n",
    "    if i % 500 == 0:\n",
    "        print(f\"{i}/{NUM_TRAINING_ITERATIONS}  Loss: {loss:.2E}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b19ce6-2da3-4e03-88ab-67e227e4156a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save weights of captured demapper\n",
    "fn = \"../../models/neural_demapper_weights_y4\"\n",
    "\n",
    "weights = neural_demapper_capture.get_weights()\n",
    "with open(fn, 'wb') as f:\n",
    "    pickle.dump(weights, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214cf3db-a1d0-43cf-8400-10f33405eb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic = True\n",
    "\n",
    "if synthetic:\n",
    "    num_inputs = 2  # 2xfloat16\n",
    "else:\n",
    "    num_inputs = 4  # 4xfloat16\n",
    "\n",
    "fn = f\"../../models/neural_demapper_weights_y{num_inputs}\"\n",
    "\n",
    "# Load weights via pickle\n",
    "with open(fn, 'rb') as f:\n",
    "    weights = pickle.load(f)\n",
    "neural_demapper = NeuralDemapper(NUM_BITS_PER_SYMBOL)\n",
    "\n",
    "# Dummy run to init layers\n",
    "neural_demapper([tf.ones((1,num_inputs))])\n",
    "\n",
    "# And load weights\n",
    "neural_demapper.set_weights(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe90562-d4c2-4d6c-bc6f-a3f1e0bfebeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs_max = 512 # max batch size for inference\n",
    "import onnx\n",
    "\n",
    "@tf.function(input_signature=input_signature)\n",
    "#def demapper_func(y):\n",
    "#    return neural_demapper(y)\n",
    "\n",
    "def demapper_func(y):\n",
    "    out = neural_demapper(y)\n",
    "    if isinstance(out, (tuple, list)):\n",
    "        out = out[0]\n",
    "    return tf.identity(out, name=\"output_1\")\n",
    "\n",
    "\n",
    "# Convert FUNCTION directly (bypasses keras.model issues)\n",
    "onnx_model, _ = tf2onnx.convert.from_function(\n",
    "    demapper_func, \n",
    "    input_signature=input_signature\n",
    ")\n",
    "onnx.checker.check_model(onnx_model)\n",
    "\n",
    "# And save the ONNX model\n",
    "onnx.save(onnx_model, f\"../../models/neural_demapper.{num_inputs}xfloat16.onnx\")\n",
    "\n",
    "m = onnx.load(f\"../../models/neural_demapper.{num_inputs}xfloat16.onnx\")\n",
    "\n",
    "#m.graph.output[0].name = \"output_1\"\n",
    "\n",
    "# Rename graph output\n",
    "old_name = m.graph.output[0].name\n",
    "m.graph.output[0].name = \"output_1\"\n",
    "\n",
    "# Rename any value_info / node outputs that reference it\n",
    "for node in m.graph.node:\n",
    "    for i, out in enumerate(node.output):\n",
    "        if out == old_name:\n",
    "            node.output[i] = \"output_1\"\n",
    "\n",
    "onnx.save(m, f\"../../models/neural_demapper.{num_inputs}xfloat16.onnx\")\n",
    "\n",
    "\n",
    "\n",
    "m = onnx.load(f\"../../models/neural_demapper.{num_inputs}xfloat16.onnx\")\n",
    "\n",
    "print(\"=== ONNX OUTPUT TENSORS ===\")\n",
    "for o in m.graph.output:\n",
    "    print(\"Output name:\", o.name)\n",
    "\n",
    "\n",
    "# And build trtengine\n",
    "trt_command = f'/usr/src/tensorrt/bin/trtexec --fp16'\n",
    "trt_command += f' --onnx=../../models/neural_demapper.{num_inputs}xfloat16.onnx'\n",
    "trt_command += f' --saveEngine=../../models/neural_demapper.{num_inputs}xfloat16.plan'\n",
    "trt_command += f' --preview=+profileSharing0806'\n",
    "trt_command += f' --inputIOFormats=fp16:chw'\n",
    "trt_command += f' --outputIOFormats=fp16:chw'\n",
    "\n",
    "trt_command += f\" --minShapes=y:1x{num_inputs}\"\n",
    "trt_command += f\" --optShapes=y:{64}x{num_inputs}\"\n",
    "trt_command += f\" --maxShapes=y:{bs_max}x{num_inputs}\"\n",
    "\n",
    "# And run the command\n",
    "os.system(trt_command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c4dd81-5880-43cf-b13f-3c3fa3559914",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ebcacb-5e03-4897-a516-f060c1d490ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python (sionna-rk)",
   "language": "python",
   "name": "sionna-rk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
